{
    "collab_server" : "",
    "contents" : "library(data.table)\nlibrary(deSolve)\nlibrary(magrittr)\nlibrary(zoo)\nlibrary(KFAS)\nlibrary(parallel)\nlibrary(tidyverse)\nlibrary(tsoutliers)\nlibrary(progress)\nlibrary(lubridate)\nlibrary(EpiEstim)\n\nget_cori <- function(df.in, \n                     icol_name, \n                     out_name = 'Cori',\n                     window = 1, \n                     SI_mean=parlist$true_mean_SI, \n                     SI_var=2*(parlist$true_mean_SI/2)^2,\n                     wend = TRUE){\n  \n  df.in[icol_name] <- na_to_0(df.in[icol_name]) ## Replace NAs in incidence\n  \n  \n  idat <- df.in %>%\n    #filter(get(icol_name) > 0 & !is.na(get(icol_name))) %>%\n    complete(time = 2:max(df.in$time)) %>%\n    mutate_all(.funs = function(xx){ifelse(is.na(xx), 0, xx)}) %>%\n    arrange(time)\n  \n  \n  \n  ts <- idat$time\n  ts <- ts[ts > 1 & ts <= (max(ts)-window+1)]\n  te <- ts+(window-1)\n  \n  estimate_R(\n    incid = pull(idat, eval(icol_name)),\n    method = \"uncertain_si\",\n    config = make_config(\n      list(\n        mean_si = SI_mean,\n        min_mean_si = SI_mean -1,\n        max_mean_si = SI_mean + 1,\n        std_mean_si = 1.5,\n        std_std_si = 1.5,\n        std_si = sqrt(SI_var),\n        min_std_si = sqrt(SI_var)*.8,\n        max_std_si = sqrt(SI_var)*1.2,\n        n1 = 50,\n        n2 = 100, \n        t_start=ts,\n        t_end=te\n      )\n    )\n  ) -> outs\n  \n  outs$R %>%\n    mutate(time = if(wend == TRUE) t_end else ceiling((t_end+t_start)/2) ) %>%\n    select(time, `Mean(R)`, `Quantile.0.025(R)`, `Quantile.0.975(R)`) %>%\n    setNames(c('time', paste0(out_name, '.mean'), paste0(out_name, '.025'), paste0(out_name, '.975')))\n}\n# Main Model function -- CALL THIS \n# dat should be a data.frame for a single region to be modeled \n#    with column \"new_confirmed\" which is count data\n#    arranged by date (increasing)\n#\n# return_fit=TRUE is for debugging mostly. \n#\n# any other columns in dat are included in output of this function. \n# \n# \n# in addition the following key variables are included as model output\n#   growth_rate == exponential growth rate\n#   pX_Y is quantile X for the quantitly Y\n#   mean_ is the mean of the quantity Y\n#   z_score_growth_rate = growth_rate/sd_growth_rate\nfit_nbss <- function(dat, precomputed_dispersions=NULL, return_fit=FALSE,maxiter=200){\n  \n  # Helper functions\n  nb_model <- function(dat, pars){\n    model_nb <- SSModel(dat$new_confirmed ~ SSMtrend(2, Q=list(0, NA),\n                                                     P1=diag(c(10, 1)),\n                                                     a1=c(0, 0),\n                                                     state_names=c(\"level\", \"trend\"))+\n                          SSMseasonal(7),\n                        u=rep(exp(pars[1]), nrow(dat)), distribution=\"negative binomial\")\n    fit <- fitSSM(model_nb, c(0), method=\"L-BFGS-B\", control=list(maxit=200, lower=-2, max=5))\n    return(fit)\n  }\n  logLik_nb <- function(dat, pars){\n    fit <- nb_model(dat, pars)\n    ll <- logLik(fit$model, marginal = TRUE)\n    return(-ll)\n  }\n  \n  # Remove preceding zeros\n  dat <- dat %>% \n    arrange(date) %>% \n    mutate(cs = cumsum(new_confirmed)) %>% \n    filter(cs > 0)\n  if (nrow(dat) < 10) return(NULL)\n  \n  # outlier detection for early outbreak\n  dat <- custom_processors(dat)\n  if (sum(dat$new_confirmed!=0) < 10) return(NULL)\n  tryCatch({\n    dat <- outlier_detection(dat)    \n  },  error = function(err){\n    return(NULL)\n  })\n  \n  if (length(dat$new_confirmed[!is.na(dat$new_confirmed)]) < 10) return(NULL)\n  \n  \n  if (!is.null(precomputed_dispersions)){\n    res <- list()\n    res$par <- dplyr::filter(precomputed_dispersions, id==unique(dat$id))$dispersion\n    if (length(res$par)==0) return(NULL) # no precomputed dispersion (previously was not able to fit likely)\n  } else {\n    res <- optim(c(-1), function(x) logLik_nb(dat, x), method=\"Brent\", lower=-2, upper=2)    \n    if (res$convergence != 0) return(NULL)\n  }\n  \n  # now fit the model with the optimized dispersion parameters\n  fit <- nb_model(dat, res$par)\n  if(return_fit) return(fit)\n  if (fit$optim.out$convergence != 0) return(NULL)\n  sm_signal <- KFS(fit$model, smoothing=\"signal\")\n  sm_state <- KFS(fit$model, smoothing=\"state\")\n  \n  out <- data.frame(p2.5_signal = exp(qnorm(0.025, sm_signal$thetahat, sqrt(c(sm_signal$V_theta)))), \n                    p97.5_signal = exp(qnorm(0.975, sm_signal$thetahat, sqrt(c(sm_signal$V_theta)))), \n                    mean_signal = exp(sm_signal$thetahat), \n                    p2.5_position = c(qnorm(0.025, sm_state$alphahat[,1], (sqrt(sm_state$V[1,1,])))), \n                    p97.5_position = c(qnorm(0.975, sm_state$alphahat[,1],(sqrt(sm_state$V[1,1,])))), \n                    mean_position = (c(sm_state$alphahat[,1])),\n                    p2.5_growth_rate = c(qnorm(0.025, sm_state$alphahat[,2], (sqrt(sm_state$V[2,2,])))), \n                    p97.5_growth_rate = c(qnorm(0.975, sm_state$alphahat[,2], (sqrt(sm_state$V[2,2,])))),\n                    p25_growth_rate = c(qnorm(0.25, sm_state$alphahat[,2], (sqrt(sm_state$V[2,2,])))), \n                    p75_growth_rate = c(qnorm(0.75, sm_state$alphahat[,2], (sqrt(sm_state$V[2,2,])))), \n                    growth_rate = (c(sm_state$alphahat[,2])),\n                    percentile_0_growth_rate =c(pnorm(0, sm_state$alphahat[,2], (sqrt(sm_state$V[2,2,])))),\n                    dispersion=res$par[1], \n                    z_score_growth_rate = c(sm_state$alphahat[,2]/sqrt(sm_state$V[2,2,])))\n  \n  if (any(grepl('administrative_area',colnames(dat)))){\n    if (any(out$p97.5_signal > 1e6)) return(NULL)\n  }\n  if (quantile(abs(out$z_score_growth_rate), probs=0.75) < 0.4) return(NULL)\n  return(cbind(dat, out))\n}\n\n\nnbss <- function(dat,level='all',mc.cores=1, precomputed_dispersions=NULL){\n  \n  if (level=='SEIR'){\n     fits <- fit_nbss(dat,precomputed_dispersions)\n  } else {\n    if (level==\"country\"){\n      tmp <- filter(dat, administrative_area_level==1)\n    } else if (level==\"state\"){\n      tmp <- filter(dat, administrative_area_level==2)\n    } else if (level==\"all\") {\n      tmp <- dat\n    } else {\n      stop(\"only level variables that are supported are all, country, and state\")\n    }\n    tmp <- dat %>% \n      as.data.frame() %>% \n      split(.$id)\n    if (mc.cores == 1){\n      fits <- list()\n      pb <- progress_bar$new(total = length(tmp), format=\" [:bar] :percent eta: :eta\")\n      for (i in 1:length(tmp)){\n        pb$tick()\n        fits[[i]] <- fit_nbss(tmp[[i]], precomputed_dispersions)\n      }  \n    } else {\n      \n      # cl <- parallel::makeCluster(mc.cores)\n      # parallel::clusterExport(cl,'custom_processors')\n      # parallel::clusterEvalQ(cl,{library(KFAS)\n      #                            library(data.table)\n      #   library(tsoutliers)\n      #   library(tidyverse)\n      #   })\n      fits <- mclapply(tmp, function(x) fit_nbss(x, precomputed_dispersions), mc.cores=mc.cores)\n    }\n    fits <- bind_rows(fits)\n  }\n  return(fits)\n}\n\n\ncustom_processors <- function(dat){\n  # Iowa and Indiana\n  # if (unique(dat$administrative_area_level_2)%in%c(\"Iowa\", \"Indiana\", \"Kentucky\")){\n  if (!\"I\" %in% colnames(dat)){\n    if (unique(dat$administrative_area_level_1)==\"United States\"){\n      if (!is.na(unique(dat$administrative_area_level_2))){\n        if (unique(dat$administrative_area_level_2 != \"Washington\")){\n          dat <- dat %>% \n            filter(date > ymd(\"2020-02-28\") )\n        }\n      }\n    }\n  } \n  return(dat)\n}\n\noutlier_detection <- function(dat){\n  \n  tryCatch({\n    res <- tso(ts(dat$new_confirmed), \n               type=\"TC\", delta=0.1, maxit.iloop = 100, maxit.oloop = 10, \n               #tsmethod = \"auto.arima\", args.tsmethod = list(allowdrift = FALSE, ic = \"bic\", stationary=TRUE),\n               tsmethod=\"arima\", args.tsmethod=list(order=c(1,1,2), method=\"ML\", transform.pars=TRUE),\n               cval=4)\n  }, error = function(err){\n    res <- tso(ts(dat$new_confirmed), \n               type=\"TC\", delta=0.1, maxit.iloop = 100, maxit.oloop = 10, \n               #tsmethod = \"auto.arima\", args.tsmethod = list(allowdrift = FALSE, ic = \"bic\", stationary=TRUE),\n               tsmethod=\"arima\", args.tsmethod=list(order=c(1,1,2), method=\"ML\", transform.pars=FALSE),\n               cval=4)\n  })\n  res <- res$outliers %>% \n    filter(tstat > 10)\n  dat <- mutate(dat, new_confirmed_nod = new_confirmed)\n  if (nrow(res)!=0){\n    dat[res$ind,\"new_confirmed\"] <- NA\n  }\n  return(dat)\n}\n\nPoissonFit <- function(n,new_confirmed,date,half_life=NULL,day_of_week,z_score=FALSE){\n  dd <- data.table('new_confirmed'=new_confirmed,\n                   'date'=date,\n                   'day_of_week'=day_of_week)\n  \n  dd <- dd[n]\n  if (!is.null(half_life)){\n    dd[,weight:=exp(as.numeric(date-min(date))*log(2)/half_life)]\n  } else {\n    dd[,weight:=1/.N]\n  }\n  \n  fit <- glm(new_confirmed~date+day_of_week,family=poisson,weights=weight,data=dd)\n  # fit <- mgcv::gam(new_confirmed~date+day_of_week,family='nb',data=dd[n])\n  if (z_score){\n    if (!'gam' %in% class(fit)){\n      \n      summary(fit)$coefficients['date','z value'] %>%\n        return\n    } else {\n      summary(fit)$p.t['date'] %>% return\n    }\n  } else {\n    return(coef(fit)['date'])\n  }\n}\n\nseird_solve <- function(t,state,parameters){\n  with(as.list(c(t,state,parameters)),{\n    if (t>lag_onset_to_death){\n      Dlag=lagvalue(t-lag_onset_to_death)[5]\n    } else {\n      Dlag=0\n    }\n    dS=lambda-(as.numeric(Dlag<=D_intervention)*beta_pre+\n                 as.numeric(Dlag>D_intervention & Dlag<=D_relaxation)*(beta_intervention)+\n                 as.numeric(Dlag>D_relaxation)*beta_relaxation)*S*I-m*S\n    dE=(as.numeric(Dlag<=D_intervention)*beta_pre+\n          as.numeric(Dlag>D_intervention & Dlag<=D_relaxation)*(beta_intervention)+\n          as.numeric(Dlag>D_relaxation)*beta_relaxation)*S*I-a*E-m*E\n    dI=a*E-m_inf*I-gamma*I\n    dR=gamma*I-m*R\n    dD=m_inf*I\n    \n    list(c(dS, dE, dI, dR, dD))\n  })\n}\n\nseird <- function(r,cfr=0.004,S0=3.27e8,start_date=as.Date('2020-01-15'),days=200,\n                  lag_onset_to_death=16,case_detection=0.01,window_size=21,half_life=NULL,\n                  intervention_efficacy=0,relaxation=1,intervention_deaths=Inf,relaxation_deaths=Inf,\n                  gamma=1/9,a=1/3,gr_estimation='nbss',\n                  day_of_week_effects=data.table('day_of_week'=c('Saturday','Sunday',\n                                                                 'Monday','Tuesday',\n                                                                 'Wednesday','Thursday','Friday'),\n                                                 'const'=c(0.6,0.4,1,1,1,1,.9))){\n  \n  state <- c('S'=S0,'E'=0,'I'=1,'R'=0,'D'=0)\n  times <- seq(0, days, by = 0.01)\n  m=8.685/100000/365\n  \n  m_inf <- cfr*gamma/(1-cfr)\n  alpha <- m_inf/m\n  \n  # alpha=1.1\n  c=a/(r+gamma+m_inf)   # the ratio of E/I during exponential phase\n  beta_pre=(r+a+m)/(c*S0)\n  beta_intervention=beta_pre*(1-intervention_efficacy)\n  beta_relaxation=beta_pre*relaxation\n  lambda=11.8/1000/365\n  parameters <- c('m'=m,\n                  'lambda'=lambda,\n                  'gamma'=gamma,\n                  'a'=a,\n                  'beta_pre'=beta_pre,\n                  'beta_intervention'=beta_intervention,\n                  'beta_relaxation'=beta_relaxation,\n                  'm_inf'=m_inf,\n                  'D_intervention'=intervention_deaths,\n                  'D_relaxation'=relaxation_deaths,\n                  'r'=r,\n                  'lag_onset_to_death'=lag_onset_to_death)\n  \n  out <- dede(y = state, times = times, func = seird_solve, parms = parameters) %>% as.data.table\n  out[,day:=ceiling(time)]\n  \n  out <- out[,list(S=S[.N],\n                   E=E[.N],\n                   I=I[.N],\n                   R=R[.N],\n                   D=D[.N]),by=day]\n  out[,date:=seq(start_date,start_date+days,by='day')]\n  out[,day_of_week:=weekdays(date)]\n  setkey(out,day_of_week)\n  setkey(day_of_week_effects,day_of_week)\n  \n  out <- day_of_week_effects[out]\n  setkey(out,date)\n  \n  \n  out[,rt:=a*E/I-m_inf-gamma]\n  out[,D:=shift(D,lag_onset_to_death)]\n  out[,beta:=(as.numeric(D<=intervention_deaths)*beta_pre+\n                as.numeric(D>intervention_deaths & D<relaxation_deaths)*(beta_intervention)+\n                as.numeric(D>=relaxation_deaths)*beta_relaxation)]\n  out[,new_infections:=beta*S*I]\n  \n  out[is.na(D),D:=0]\n  out[,new_deaths:=c(0,diff(D))]\n  \n  out[,case_detection:=case_detection]\n  \n  out[,new_confirmed:=rpois(.N,const*case_detection*I)]\n  out[,n:=1:.N]\n  if (gr_estimation=='nbss'){\n    out <- nbss(out,level='SEIR')\n    out[,growth_rate:=shift(growth_rate,lag_onset_to_death)]\n  } else {\n    out[,growth_rate:=rollapply(n,width=window_size,FUN=PoissonFit,fill=NA,\n                              new_confirmed=new_confirmed,date=date,half_life=half_life,\n                              day_of_week=day_of_week,align='right')]\n  }\n  out$n <- NULL\n  out[,r:=r]\n  out[,cfr:=cfr]\n  out[,lag_onset_to_death:=lag_onset_to_death]\n  out[,intervention_efficacy:=intervention_efficacy]\n  out[,intervention_deaths:=intervention_deaths]\n  out[,relaxation:=relaxation]\n  out[,relaxation_deaths:=relaxation_deaths]\n  out[,deaths_pc:=D/S0]\n  \n  return(out)\n}\n",
    "created" : 1598384528156.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "900624451",
    "id" : "D9856A96",
    "lastKnownWriteTime" : 1598465562,
    "last_content_update" : 1598465562327,
    "path" : "~/COVID/covid_r_v_D/scripts/utils.R",
    "project_path" : "scripts/utils.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}